{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a16b708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T16:35:23.995345Z",
     "iopub.status.busy": "2023-12-20T16:35:23.994675Z",
     "iopub.status.idle": "2023-12-20T16:35:28.912954Z",
     "shell.execute_reply": "2023-12-20T16:35:28.911666Z"
    },
    "papermill": {
     "duration": 4.931488,
     "end_time": "2023-12-20T16:35:28.916454",
     "exception": false,
     "start_time": "2023-12-20T16:35:23.984966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import warnings  # Handling warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "#tuning hyperparameters\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# ðŸ¤ Disable warnings to keep the code clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ðŸ“Š Define flags and variables\n",
    "is_train = False  # Flag for training mode\n",
    "is_infer = False\n",
    "\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6edc1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T16:35:28.935620Z",
     "iopub.status.busy": "2023-12-20T16:35:28.935176Z",
     "iopub.status.idle": "2023-12-20T16:35:28.950779Z",
     "shell.execute_reply": "2023-12-20T16:35:28.949509Z"
    },
    "papermill": {
     "duration": 0.027014,
     "end_time": "2023-12-20T16:35:28.953788",
     "exception": false,
     "start_time": "2023-12-20T16:35:28.926774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8275f47e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T16:35:28.969483Z",
     "iopub.status.busy": "2023-12-20T16:35:28.969013Z",
     "iopub.status.idle": "2023-12-20T16:35:55.628015Z",
     "shell.execute_reply": "2023-12-20T16:35:55.626460Z"
    },
    "papermill": {
     "duration": 26.671038,
     "end_time": "2023-12-20T16:35:55.631498",
     "exception": false,
     "start_time": "2023-12-20T16:35:28.960460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 234.78 Mb (65.4% reduction)\n",
      "Shape of df:  (5237892, 17)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n",
    "# ðŸ§¹ Remove rows with missing values in the \"target\" column\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "\n",
    "# ðŸ” Reset the index of the DataFrame and apply the changes in place\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = reduce_mem_usage(df)\n",
    "print(\"Shape of df: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60a26d",
   "metadata": {
    "papermill": {
     "duration": 0.006332,
     "end_time": "2023-12-20T16:35:55.645633",
     "exception": false,
     "start_time": "2023-12-20T16:35:55.639301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "480e9faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T16:35:55.661514Z",
     "iopub.status.busy": "2023-12-20T16:35:55.661015Z",
     "iopub.status.idle": "2023-12-20T16:35:55.686154Z",
     "shell.execute_reply": "2023-12-20T16:35:55.684768Z"
    },
    "papermill": {
     "duration": 0.036747,
     "end_time": "2023-12-20T16:35:55.689269",
     "exception": false,
     "start_time": "2023-12-20T16:35:55.652522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_eng(data):\n",
    "    drop_cols = []\n",
    "    if 'row_id' in data.columns:\n",
    "        drop_cols.append('row_id')\n",
    "    if 'time_id' in data.columns:\n",
    "        drop_cols.append('time_id')\n",
    "    df=data.drop(drop_cols, axis=1)\n",
    "    df['trade_volume'] = df['bid_size'] + df['ask_size']\n",
    "    df['trade_ratio'] = df['bid_size'] / df['ask_size']\n",
    "    df['trade_volume_diff'] = df['bid_size'] - df['ask_size']\n",
    "    df['imbalance_ratio_1'] = df['imbalance_size'] / df['trade_volume']\n",
    "    df['imbalance_ratio_2'] = df['imbalance_size'] / df['trade_volume_diff']\n",
    "    df['imbalance_ratio_3'] = df['imbalance_size'] / df['matched_size']\n",
    "    df['mid_price'] = (df['bid_price'] + df['ask_price']) / 2\n",
    "    df['price_spread'] = df['bid_price'] - df['ask_price']\n",
    "    df['far_near_spread'] = df['far_price'] - df['near_price']\n",
    "    df['price_spread_ratio'] = df['price_spread'] / df['far_near_spread']\n",
    "    \n",
    "    # stock trading price max, min, std, med\n",
    "    # trading volume max, min, std, med\n",
    "    # diff price vs med\n",
    "    # diff trading volume vs med\n",
    "    stats_cols = [\n",
    "        'trade_volume', 'wap', 'reference_price', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size'\n",
    "    ]\n",
    "    df_by_stock = df[stats_cols+['stock_id']].groupby('stock_id').agg(['mean', 'std'])\n",
    "    df_by_stock.columns = [\"_\".join(x) for x in df_by_stock.columns.ravel()]\n",
    "    df = df.merge(df_by_stock, on='stock_id', how='left')\n",
    "    for col in [x for x in stats_cols if 'price' in x]:\n",
    "        df['wap_' + col+'_'+'diff'] = df['wap']-df[col]\n",
    "        df[col+'_mean_diff'] = df[col] - df[col+'_mean']\n",
    "        df[col+'_mean_diff_normed'] = df[col+'_mean_diff']/df[col+'_std']\n",
    "    \n",
    "    for col in ['wap', 'trade_volume', 'bid_size', 'ask_size']:\n",
    "        df[col +'_mean_diff'] = df[col] - df[col+'_mean']\n",
    "        df[col+'_mean_diff_normed'] = df[col+'_mean_diff'] / df[col+'_std']\n",
    "        \n",
    "    for col in stats_cols:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "    # avg/diff for same day and stock\n",
    "    day_cols = stats_cols + ['imbalance_size', 'matched_size']\n",
    "    df_by_stock_date = df[day_cols+['stock_id','date_id']].groupby(['stock_id', 'date_id']).mean()\n",
    "    df_by_stock_date.columns = [x+'_day_mean' for x in day_cols]\n",
    "    fst_by_stock_date = (df[day_cols+['stock_id','date_id', 'seconds_in_bucket']]\n",
    "                         .sort_values(['stock_id','date_id', 'seconds_in_bucket'])\n",
    "                         .groupby(['stock_id', 'date_id']).first()\n",
    "                        )\n",
    "    fst_by_stock_date.columns.values[:-1] = [x+'_day_start' for x in day_cols]\n",
    "    df = (df\n",
    "          .merge(fst_by_stock_date, on=['stock_id','date_id'], how='left')\n",
    "          .merge(df_by_stock_date, on=['stock_id','date_id'], how='left')\n",
    "         )\n",
    "    for col in day_cols:\n",
    "        df[col+'_diff_start'] = df[col] - df[col+'_day_start']\n",
    "        df[col+'_diff_day_mean'] = df[col] - df[col+'_day_mean']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56a9f84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T16:35:55.704713Z",
     "iopub.status.busy": "2023-12-20T16:35:55.703670Z",
     "iopub.status.idle": "2023-12-20T16:36:28.870033Z",
     "shell.execute_reply": "2023-12-20T16:36:28.868379Z"
    },
    "papermill": {
     "duration": 33.17757,
     "end_time": "2023-12-20T16:36:28.873380",
     "exception": false,
     "start_time": "2023-12-20T16:35:55.695810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = feature_eng(df)\n",
    "y_train = df['target']\n",
    "X_train.drop('target', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13eec76",
   "metadata": {
    "papermill": {
     "duration": 0.006153,
     "end_time": "2023-12-20T16:36:28.886659",
     "exception": false,
     "start_time": "2023-12-20T16:36:28.880506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be9d6c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T16:36:28.902320Z",
     "iopub.status.busy": "2023-12-20T16:36:28.901819Z",
     "iopub.status.idle": "2023-12-20T16:36:28.929113Z",
     "shell.execute_reply": "2023-12-20T16:36:28.927357Z"
    },
    "papermill": {
     "duration": 0.038589,
     "end_time": "2023-12-20T16:36:28.931625",
     "exception": false,
     "start_time": "2023-12-20T16:36:28.893036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.925638165305344, {'bagging_fraction': 0.8611136941833629, 'feature_fraction': 0.5735009541004218, 'learning_rate': 0.6919300183194385, 'max_bin': 76, 'max_depth': 26, 'min_data_in_leaf': 64, 'min_sum_hessian_in_leaf': 81.39493869656881, 'num_leaves': 176, 'subsample': 0.08833040944871326, 'objective': 'regression', 'metric': 'mae', 'n_estimators': 3000, 'reg_alpha': 0.1})\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter optimization\n",
    "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=15, n_folds=4, random_seed=6, output_process=False):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(\n",
    "        learning_rate, num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,\n",
    "        min_sum_hessian_in_leaf,subsample, n_estimators, reg_alpha\n",
    "    ):\n",
    "        params = {'objective': 'regression', 'metric':'mae'}\n",
    "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['max_bin'] = int(round(max_bin))\n",
    "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "#         params['n_estimators'] = int(round(n_estimators)*100)\n",
    "#         params['reg_alpha'] = reg_alpha\n",
    "#         scores = cross_val_score(lgb.LGBMRegressor(random_state=random_seed, **params),\n",
    "#                              X_train, y_train, scoring='neg_mean_absolute_error', cv=n_folds).mean()\n",
    "#         return scores.mean()\n",
    "        \n",
    "        cv_result = lgb.cv(\n",
    "            params, train_data, nfold=5, seed=random_seed, stratified=False, metrics=['l1']\n",
    "        )\n",
    "        return max(cv_result['l1-mean'])\n",
    "\n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n",
    "                                            'num_leaves': (64, 256),\n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'bagging_fraction': (0.8, 1),\n",
    "                                            'max_depth': (10, 30),\n",
    "                                            'max_bin':(30,90),\n",
    "                                            'min_data_in_leaf': (20, 80),\n",
    "                                            'min_sum_hessian_in_leaf':(10,90),\n",
    "                                            'subsample': (0.01, 1.0),\n",
    "                                            'n_estimators': (5, 10),\n",
    "                                            'reg_alpha': (0.01, 1.0)\n",
    "                                           })\n",
    "\n",
    "\n",
    "    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n",
    "\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "\n",
    "    model_mae=[]\n",
    "    for model in range(len(lgbBO.res)):\n",
    "        model_mae.append(lgbBO.res[model]['target'])\n",
    "\n",
    "    # return best parameters\n",
    "    return lgbBO.res[pd.Series(model_mae).idxmin()]['target'],lgbBO.res[pd.Series(model_mae).idxmin()]['params']\n",
    "\n",
    "if is_train:\n",
    "    opt_params = bayes_parameter_opt_lgb(X_train, y_train, n_folds=3)\n",
    "else:\n",
    "    # previously saved opt_params \n",
    "    opt_params = (5.925638165305344, {'bagging_fraction': 0.8611136941833629, 'feature_fraction': 0.5735009541004218, 'learning_rate': 0.6919300183194385, 'max_bin': 76, 'max_depth': 26, 'min_data_in_leaf': 64, 'min_sum_hessian_in_leaf': 81.39493869656881, 'num_leaves': 176, 'subsample': 0.08833040944871326, 'objective': 'regression', 'metric': 'mae', 'n_estimators': 3000, 'reg_alpha':0.1,})\n",
    "\n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='regression'\n",
    "opt_params[1]['metric']='mae'\n",
    "\n",
    "# k fold, best model\n",
    "print(opt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6d003",
   "metadata": {
    "papermill": {
     "duration": 0.005964,
     "end_time": "2023-12-20T16:36:28.944065",
     "exception": false,
     "start_time": "2023-12-20T16:36:28.938101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Time series cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b9dfd0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T16:36:28.959562Z",
     "iopub.status.busy": "2023-12-20T16:36:28.958703Z",
     "iopub.status.idle": "2023-12-20T16:36:28.967380Z",
     "shell.execute_reply": "2023-12-20T16:36:28.966028Z"
    },
    "papermill": {
     "duration": 0.019741,
     "end_time": "2023-12-20T16:36:28.970254",
     "exception": false,
     "start_time": "2023-12-20T16:36:28.950513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_series_cross_validation_split(num_folds, date_list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_folds : int\n",
    "      The number of folds for which to split the data\n",
    "    date_list : list\n",
    "    \"\"\"\n",
    "    # Define percentages on which to split data based on rank and number of folds\n",
    "    date_list = np.sort(date_list)\n",
    "    date_pct_rank = np.array(date_list/len(date_list))\n",
    "    fold_percentage = 1 / (num_folds + 2)\n",
    "    train_dates = []\n",
    "    valid_dates = []\n",
    "    # For each fold\n",
    "    for i in range(2, num_folds + 1):\n",
    "        train_set = date_list[date_pct_rank<=fold_percentage * i]\n",
    "        valid_set = date_list[(date_pct_rank<=fold_percentage * (i+1))&(date_pct_rank>fold_percentage * i)]\n",
    "        \n",
    "        # Append to lists to return \n",
    "        train_dates.append(train_set)\n",
    "        valid_dates.append(valid_set)\n",
    "    return train_dates, valid_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a417d9e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T16:36:28.986279Z",
     "iopub.status.busy": "2023-12-20T16:36:28.984514Z",
     "iopub.status.idle": "2023-12-20T16:36:33.730424Z",
     "shell.execute_reply": "2023-12-20T16:36:33.728372Z"
    },
    "papermill": {
     "duration": 4.756996,
     "end_time": "2023-12-20T16:36:33.733552",
     "exception": false,
     "start_time": "2023-12-20T16:36:28.976556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5735009541004218, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5735009541004218\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=64, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=64\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=81.39493869656881, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=81.39493869656881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8611136941833629, subsample=0.08833040944871326 will be ignored. Current value: bagging_fraction=0.8611136941833629\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\tvalid_0's l1: 6.26756\n",
      "[200]\tvalid_0's l1: 6.26756\n",
      "[300]\tvalid_0's l1: 6.26756\n",
      "[400]\tvalid_0's l1: 6.26756\n",
      "[500]\tvalid_0's l1: 6.26756\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 6.26756\n",
      "Fold 0 MAE: 6.251924886144627\n",
      "Fold 1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5735009541004218, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5735009541004218\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=64, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=64\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=81.39493869656881, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=81.39493869656881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8611136941833629, subsample=0.08833040944871326 will be ignored. Current value: bagging_fraction=0.8611136941833629\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\tvalid_0's l1: 6.50652\n",
      "[200]\tvalid_0's l1: 6.68332\n",
      "[300]\tvalid_0's l1: 6.63883\n",
      "[400]\tvalid_0's l1: 6.52791\n",
      "[500]\tvalid_0's l1: 6.56394\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's l1: 5.65946\n",
      "Fold 1 MAE: 5.659458591068179\n",
      "Fold 2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5735009541004218, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5735009541004218\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=64, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=64\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=81.39493869656881, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=81.39493869656881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8611136941833629, subsample=0.08833040944871326 will be ignored. Current value: bagging_fraction=0.8611136941833629\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\tvalid_0's l1: 5.18479\n",
      "[200]\tvalid_0's l1: 4.97027\n",
      "[300]\tvalid_0's l1: 4.92639\n",
      "[400]\tvalid_0's l1: 4.75576\n",
      "[500]\tvalid_0's l1: 4.7089\n",
      "[600]\tvalid_0's l1: 4.68719\n",
      "[700]\tvalid_0's l1: 4.67588\n",
      "[800]\tvalid_0's l1: 4.66695\n",
      "[900]\tvalid_0's l1: 4.66892\n",
      "[1000]\tvalid_0's l1: 4.65597\n",
      "[1100]\tvalid_0's l1: 4.64476\n",
      "[1200]\tvalid_0's l1: 4.63579\n",
      "[1300]\tvalid_0's l1: 4.64086\n",
      "[1400]\tvalid_0's l1: 4.63458\n",
      "[1500]\tvalid_0's l1: 4.63714\n",
      "[1600]\tvalid_0's l1: 4.63523\n",
      "[1700]\tvalid_0's l1: 4.63719\n",
      "[1800]\tvalid_0's l1: 4.63795\n",
      "[1900]\tvalid_0's l1: 4.6363\n",
      "Early stopping, best iteration is:\n",
      "[1416]\tvalid_0's l1: 4.63289\n",
      "Fold 2 MAE: 4.632892192081873\n",
      "Fold 3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5735009541004218, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5735009541004218\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=64, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=64\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=81.39493869656881, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=81.39493869656881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8611136941833629, subsample=0.08833040944871326 will be ignored. Current value: bagging_fraction=0.8611136941833629\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\tvalid_0's l1: 4.55399\n",
      "[200]\tvalid_0's l1: 4.64789\n",
      "[300]\tvalid_0's l1: 4.70744\n",
      "[400]\tvalid_0's l1: 4.64742\n",
      "[500]\tvalid_0's l1: 4.63056\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's l1: 4.40219\n",
      "Fold 3 MAE: 4.402190837189111\n",
      "CV Average MAE: 5.23662 \n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros(len(y_train))\n",
    "scores = []\n",
    "feature_importance_df = pd.DataFrame()\n",
    "date_arr = np.array(X_train['date_id'].unique())\n",
    "train, valid = time_series_cross_validation_split(5, date_arr)\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(zip(train, valid)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    reg = lgb.LGBMRegressor(**opt_params[1])\n",
    "    reg.fit(\n",
    "        X_train.iloc[trn_idx],\n",
    "        y_train.iloc[trn_idx],\n",
    "        eval_set=[(X_train.iloc[val_idx], y_train.iloc[val_idx])],\n",
    "        callbacks=[\n",
    "            lgb.callback.early_stopping(stopping_rounds=500),\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "    preds[val_idx] = reg.predict(X_train.iloc[val_idx])\n",
    "#     reg = lgb.train(opt_params[1], trn_data, num_round, valid_sets = [trn_data, val_data])\n",
    "#     preds[val_idx] = reg.predict(X_train.iloc[val_idx], num_iteration=reg.best_iteration)\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = X_train.columns\n",
    "    fold_importance_df[\"importance\"] = reg.feature_importances_ #feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    fold_score = mean_absolute_error(preds[val_idx], y_train[val_idx])\n",
    "    scores.append(fold_score)\n",
    "    print(f\"Fold {fold_} MAE: {fold_score}\")\n",
    "\n",
    "print(\"CV Average MAE: {:<8.5f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969863ca",
   "metadata": {
    "papermill": {
     "duration": 0.009179,
     "end_time": "2023-12-20T16:36:33.752129",
     "exception": false,
     "start_time": "2023-12-20T16:36:33.742950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a3f976",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T16:36:33.773753Z",
     "iopub.status.busy": "2023-12-20T16:36:33.773305Z",
     "iopub.status.idle": "2023-12-20T16:36:33.781154Z",
     "shell.execute_reply": "2023-12-20T16:36:33.779769Z"
    },
    "papermill": {
     "duration": 0.021358,
     "end_time": "2023-12-20T16:36:33.783309",
     "exception": false,
     "start_time": "2023-12-20T16:36:33.761951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    # plot feature importance, feature selection\n",
    "    cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "            .groupby(\"Feature\")\n",
    "            .mean()\n",
    "            .sort_values(by=\"importance\", ascending=False)[:20].index)\n",
    "    best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(20,28))\n",
    "    sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "    plt.title('Features importance (averaged/folds)')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46f931",
   "metadata": {
    "papermill": {
     "duration": 0.008881,
     "end_time": "2023-12-20T16:36:33.801923",
     "exception": false,
     "start_time": "2023-12-20T16:36:33.793042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b77c12ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T16:36:33.822485Z",
     "iopub.status.busy": "2023-12-20T16:36:33.822095Z",
     "iopub.status.idle": "2023-12-20T16:36:33.837058Z",
     "shell.execute_reply": "2023-12-20T16:36:33.835658Z"
    },
    "papermill": {
     "duration": 0.029034,
     "end_time": "2023-12-20T16:36:33.840254",
     "exception": false,
     "start_time": "2023-12-20T16:36:33.811220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "\n",
    "if is_infer:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps, predictions = [], []\n",
    "    cache = pd.DataFrame()\n",
    "    \n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        feat = feature_eng(cache)[-len(test):]\n",
    "\n",
    "        # added after new API, reference: https://www.kaggle.com/competitions/optiver-trading-at-the-close/discussion/455690#2526672\n",
    "#         if test.currently_scored.iloc[0]== False:\n",
    "#             print('test.currently_scored')\n",
    "#             sample_prediction['target'] = reg.predict(feat)\n",
    "#             env.predict(sample_prediction)\n",
    "#             counter += 1\n",
    "#             qps.append(time.time() - now_time)\n",
    "#             if counter % 10 == 0:\n",
    "#                 print(counter, 'qps:', np.mean(qps))\n",
    "#             continue\n",
    "\n",
    "        feat = feat.drop(columns = [\"currently_scored\"])    \n",
    "        # end of new codes for new API\n",
    "        \n",
    "        # Generate predictions for each model and calculate the weighted average\n",
    "        lgb_predictions = reg.predict(feat)\n",
    "\n",
    "        lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n",
    "        clipped_predictions = np.clip(lgb_predictions, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        print(sample_prediction)\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09e923",
   "metadata": {
    "papermill": {
     "duration": 0.008816,
     "end_time": "2023-12-20T16:36:33.858336",
     "exception": false,
     "start_time": "2023-12-20T16:36:33.849520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 75.888306,
   "end_time": "2023-12-20T16:36:34.992831",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-20T16:35:19.104525",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
